TARANTULA IMPLEMENTATION:
For my implementation of tarantula I needed to find a bug that only happened every so often with my random tester. I found that Mckenna Jones’ dominion.c file failed steward about 50% percent of the time, while my code passed the tests 100% of the time. This made his code the perfect candidate for the implementing tarantula with. I’m going to take 12 outputs of the gcov after running my steward random tester, with some fails and some successes, and use tarantula to see what lines appeared to have been executed during fails the most, in order to localize this fault. 

To implement tarantula I ran my random tester for steward against the failing dominion code using random seeds initialized to time(null). My goal is to be able to replicate tests where the code fails and where the code succeeds because in order to have tarantula work we need to have some successes and some fails. I printed out the seeds and documented the 6 seeds that failed and 6 seeds that succeeded. I ran my tarantula against the 12 total test case’s gcov output and printed out pass or fail text based on if it passed or failed. My tarantula would catch these pass/fail texts inside the code output and use it to calculate the formula for suspicious code lines. This way I can determine what code appears frequently in failing tests but not in passing tests. 

The most difficult part of tarantula was finding a bug I could properly implement it on. Most of the bugs I found in mine or others code is bugs that resulted in a failed test every time. This is not useful for tarantula because it does not allow us to eliminate possible non suspicious lines that appear in successful test cases.

Once I outputted each test case gcov to their own file, I concatenated them all into one giant test output file. Each gcov is preceded by their test status (either a P for pass or a F for fail). The complete test cases output is in testout.out, while all the separate non combined files are in testout1,testout2 etc. The programs themselves that created these outputs are named testcase1, testcase2 etc.

The code can be run by typing “python tarantula.py testcase.out”

Output shows line numbers on the left and suspiciousness levels to the right of the line numbers. It ranges from 0 to 1, where 1 is the more suspicious.

FINDINGS:
While tarantula took a long time for me to get implemented correctly, the fault localization worked perfectly. By looking at the tarantula output I was able to focus my attention to lines that close to 1 in suspiciousness level. 4 lines stood out to me with a suspiciousness level of 1. Once I honed in on this area of the code it became apparent what the bug was. Mckenna did not correctly implement the refactoring of the cases. Cases must have a return value or else the compiler does not know when the end of the test case is. This dominion code called the refactored functions but did not return them. The failed test cases were caused by the steward case “leaking” into the functionality of tribute and setting off some of its if statements, but only in certain situations. This is why tarantula was useful. Since this bug created a fault only some of the time, its hard to try patterns and conclusions as to what is wrong. One might wrongfully try to find the bug in the steward function. Tarantula gave me a clear view as to what code was exclusively being called when the test cases failed. 